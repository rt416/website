+++
abstract = "Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs), a model that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). ANTs allow increased interpretability via hierarchical clustering, e.g., learning meaningful class associations, such as separating natural vs. man-made objects. We demonstrate this on classification and regression tasks, achieving over 99% and 90% accuracy on the MNIST and CIFAR-10 datasets, and outperforming standard neural networks, random forests and gradient boosted trees on the SARCOS dataset. Furthermore, ANT optimisation naturally adapts the architecture to the size and complexity of the training data."
abstract_short = "We propose a framework to combine decision trees and neural networks, and show on image classification tasks that it enjoys the complementary benefits of the two approaches, while addressing the limitations of prior work."
authors = ["Ryutaro Tanno", "Kai Arulkumaran", "Daniel C. Alexander", "Antonio Criminisi", "Aditya Nori"]
date = "2018-11-18"
image = "./../images/ant.png"
image_preview = ""
math = false
publication = "*Preprint* "
selected = true
title = " Adaptive Neural Trees"
publication_types = ["1"]
url_dataset = ""
url_pdf = "pdf/tanno_ants_2018.pdf"

#[[url_custom]]
#name = "Poster"
#url = "pdf/tanno_miccai_2016_poster.pdf"
#url_project = ""

[[url_custom]]
name = "Link"
url="https://arxiv.org/abs/1807.06699"

[[social_media]]
name = "twitter"
url = "https://twitter.com/kailasharul/status/1019736584189612032?lang=en"
+++
