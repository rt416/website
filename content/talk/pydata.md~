+++
date = "2019-06-31T00:00:00"
title = "Modelling human uncertainty: how to teach machines when experts disagree with each other"
#abstract = "Access to clean and voluminous datasets is a piece of luxury confined to academic research for many machine learning applications. In practice, such datasets are hard to come by, and consequently limit the performance of deployed machine learning systems. This problem is pervasive in medical imaging applications where the cost of data acquisition and labelling is high. In this talk, I will present a method that is capable of learning more intelligently from such noisy data by modelling the human annotation process. This is particularly relevant in situations where data is labelled by multiple annotators of varying skill levels and biases. "
abstract = "The predictive performance of supervised learning algorithms depends on the quality of labels. In a typical label collection process, multiple annotators provide subjective noisy estimates of the “truth” under the influence of their varying skill-levels and biases. Blindly treating these noisy labels as the ground truth limits the accuracy of learning algorithms in the presence of strong disagreement. This problem is critical for applications in domains such as medical imaging where both the annotation cost and inter-observer variability are high. In this work, we present a method for simultaneously learning the individual annotator model and the underlying true label distribution, using only noisy observations. Each annotator is modeled by a confusion matrix that is jointly estimated along with the classifier predictions. We propose to add a regularisation term to the loss function that encourages convergence to the true annotator confusion matrix. We provide a theoretical argument as to how the regularisation is essential to our approach both for the case of single annotator and multiple annotators. Despite the simplicity of the idea, experiments on image classification tasks with both simulated and real labels show that our method either outperforms or performs on par with the state-of-the-art methods and is capable of estimating the skills of annotators even with a single label available per image."
abstract_short = ""
event = "PyData Cambridge Meetup"
event_url = "https://www.meetup.com/PyData-Cambridge-Meetup/events/263237446/comments/503467522/?read=1&_xtd=gatlbWFpbF9jbGlja9oAJDc3ZmFhZDA3LWUwMzItNDIxYi1hYmRiLTNiZDcyYWU3ZDBjNQ&_af=event&_af_eid=263237446&itemTypeToken=COMMENT" 
location = "Cambridge, UK"

selected = false
math = true

url_pdf = ""
url_slides = ""
url_video = ""

# Optional featured image (relative to `static/img/` folder).
#[header]
#image = "headers/bubbles-wide.jpg"
#caption = "My caption :smile:"

+++
Invited by [Ole Schulz-Trieglaff](https://uk.linkedin.com/in/oschulztrieglaff)
